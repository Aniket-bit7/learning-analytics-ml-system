{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vipW_DbzLCrp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Aniket-bit7/learning-analytics-ml-system/main/data/students_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Final_Result\"].unique()"
      ],
      "metadata": {
        "id": "1K5YiymvC8jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", df.shape)"
      ],
      "metadata": {
        "id": "yVIkKOlvAqL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pZOClY32Apc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping Student_ID from modeling as it has no predictive meaning and it may introduce noise\n",
        "df = df.drop(columns=[\"Student_ID\"])"
      ],
      "metadata": {
        "id": "I_VXsoa9DVQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking missing values -->\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "lW5cx8AKBauP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking duplicates -->\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "l7ujkB_CBaro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Mlt3_5Q4Bao5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization part -->\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df.hist(figsize=(12, 8))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eEi1oQDBFQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"Final_Result\", data=df)\n",
        "plt.title(\"Final Result Distribution\")\n",
        "plt.show()\n",
        "\n",
        "df[\"Final_Result\"].value_counts()"
      ],
      "metadata": {
        "id": "ZOD2byLMFbks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"Final_Result\", y=\"Attendance\", data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGRJs_hwI_58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Values\n"
      ],
      "metadata": {
        "id": "kMjqpfYwN_x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before handling missing values -->\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df[\"Quiz2\"].fillna(df[\"Quiz2\"].mean(), inplace=True)\n",
        "df[\"Time_Spent\"].fillna(df[\"Time_Spent\"].mean(), inplace=True)\n",
        "df[\"Attendance\"].fillna(df[\"Attendance\"].mean(), inplace=True)\n",
        "\n",
        "# after handling missing values -->\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "uzLdSOytI_pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Handling"
      ],
      "metadata": {
        "id": "V2xagEMa3q6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df[\"Time_Spent\"].quantile(0.25)\n",
        "Q3 = df[\"Time_Spent\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df[\"Time_Spent\"] = df[\"Time_Spent\"].clip(lower_bound, upper_bound)"
      ],
      "metadata": {
        "id": "G4lbdyO63wmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=df[\"Time_Spent\"])\n",
        "plt.title(\"Time_Spent After Outlier Handling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "89Anco9J4K5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding and visualization"
      ],
      "metadata": {
        "id": "PsMciOTdP8qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Final_Result' column to numerical values\n",
        "df['Final_Result'] = df['Final_Result'].map({'Pass': 1, 'Fail': 0})\n",
        "\n",
        "# correlation heatmap -->\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BTeoO1tUP69c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "gEgORr0US0cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total Quiz Score -->\n",
        "df[\"Total_Quiz_Score\"] = df[\"Quiz1\"] + df[\"Quiz2\"] + df[\"Quiz3\"]"
      ],
      "metadata": {
        "id": "T-7IfPceS4VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average Quiz Score -->\n",
        "df[\"Average_Quiz_Score\"] = df[\"Total_Quiz_Score\"] / 3"
      ],
      "metadata": {
        "id": "unk9qA0iS4RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz_Consistency -->\n",
        "df[\"Quiz_Std\"] = df[[\"Quiz1\", \"Quiz2\", \"Quiz3\"]].std(axis=1)"
      ],
      "metadata": {
        "id": "-9t22yGOTFF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nc-5MpGBUl3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average Quiz Score -->\n",
        "df[\"Average_Quiz_Score\"] = df[\"Total_Quiz_Score\"] / 3"
      ],
      "metadata": {
        "id": "H6z2jmOpzKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz_Consistency -->\n",
        "df[\"Quiz_Std\"] = df[[\"Quiz1\", \"Quiz2\", \"Quiz3\"]].std(axis=1)"
      ],
      "metadata": {
        "id": "0iUX9GrZzKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engagement_Index -->\n",
        "df[\"Engagement_Index\"] = (\n",
        "    df[\"Time_Spent\"] * 0.5 +\n",
        "    df[\"Assignments\"] * 0.3 +\n",
        "    df[\"Attendance\"] * 0.2\n",
        ")"
      ],
      "metadata": {
        "id": "n_aNBV-4zKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "7mytXKQFUl1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Ratio -->\n",
        "df[\"Quiz_Percentage\"] = (df[\"Total_Quiz_Score\"] / 300) * 100"
      ],
      "metadata": {
        "id": "l319YLz4UlyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Effort_Performance_Ratio -->\n",
        "df[\"Effort_Performance_Ratio\"] = (\n",
        "    df[\"Total_Quiz_Score\"] / (df[\"Time_Spent\"] + 1)\n",
        ")"
      ],
      "metadata": {
        "id": "NzWoIcXWUluJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset"
      ],
      "metadata": {
        "id": "gJZc7R6zzNrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[\"Final_Result\"])\n",
        "y = df[\"Final_Result\"]"
      ],
      "metadata": {
        "id": "DGa7l6BhzT4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "DO536tbXzT1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original distribution:\\n\", y.value_counts(normalize=True))\n",
        "print(\"\\nTraining distribution:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"\\nTest distribution:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "2hDcqPbpzTyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale Features"
      ],
      "metadata": {
        "id": "N9DJhsXp-p-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n"
      ],
      "metadata": {
        "id": "F8a6JZG4-utB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7_bcIwZ5-uqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING\n"
      ],
      "metadata": {
        "id": "aFaICeOH_nQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using Logistic regression for training model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "3OEuvFxT-uKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions -->\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]"
      ],
      "metadata": {
        "id": "5oLwxagM-uHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics -->\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "id": "C08GfaC5AMFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Class Imbalance Properly -->\n",
        "log_reg_balanced = LogisticRegression(\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg_balanced.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "GPkT26KSAh2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again evaluating -->\n",
        "y_pred_bal = log_reg_balanced.predict(X_test_scaled)\n",
        "y_prob_bal = log_reg_balanced.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Confusion Matrix (Balanced):\\n\", confusion_matrix(y_test, y_pred_bal))\n",
        "print(\"\\nClassification Report (Balanced):\\n\", classification_report(y_test, y_pred_bal))\n",
        "print(\"ROC-AUC Score (Balanced):\", roc_auc_score(y_test, y_prob_bal))"
      ],
      "metadata": {
        "id": "IrIGZ5ZLAlyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "Q-Oc3uHcEJqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = log_reg_balanced"
      ],
      "metadata": {
        "id": "xTeTmpQdCJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = final_model.predict(X_test_scaled)\n",
        "y_prob = final_model.predict_proba(X_test_scaled)[:, 1]"
      ],
      "metadata": {
        "id": "LF-QxcjuCLek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "Wg1VXE3FDE8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O0ugLi7-DHMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "U-2lO6NtDJBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision (Pass):\", precision)\n",
        "print(\"Recall (Pass):\", recall)\n",
        "print(\"F1-score (Pass):\", f1)"
      ],
      "metadata": {
        "id": "1CEFlMVoDPH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "lyJjspzgDWLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0,1], [0,1], linestyle='--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FKhBQDKTDYj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans Clustering\n"
      ],
      "metadata": {
        "id": "UEoc3UOsFFmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_features = df[[\n",
        "    \"Average_Quiz_Score\",\n",
        "    \"Engagement_Index\",\n",
        "    \"Attendance\"\n",
        "]]"
      ],
      "metadata": {
        "id": "Rdm7OtTLFLUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_cluster = StandardScaler()\n",
        "cluster_scaled = scaler_cluster.fit_transform(cluster_features)"
      ],
      "metadata": {
        "id": "Lf4hN7-BFLR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "clusters = kmeans.fit_predict(cluster_scaled)\n",
        "\n",
        "df[\"Cluster\"] = clusters"
      ],
      "metadata": {
        "id": "EySyYKN1FPIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_summary = df.groupby(\"Cluster\")[[\n",
        "    \"Average_Quiz_Score\",\n",
        "    \"Engagement_Index\",\n",
        "    \"Attendance\"\n",
        "]].mean()\n",
        "\n",
        "print(cluster_summary)"
      ],
      "metadata": {
        "id": "GhSaghfmFQka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cluster_Label\"] = df[\"Cluster\"].map({\n",
        "    0: \"At Risk (Low Engagement)\",\n",
        "    1: \"High Performer\",\n",
        "    2: \"Struggling but Engaged\"\n",
        "})"
      ],
      "metadata": {
        "id": "F0PvL2PGFSDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cluster_Label\"].value_counts()"
      ],
      "metadata": {
        "id": "oqtCWqy7F7vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Cluster_Label\"].value_counts(normalize=True) * 100"
      ],
      "metadata": {
        "id": "7kktA9WPF7sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df[\"Cluster_Label\"], df[\"Final_Result\"])"
      ],
      "metadata": {
        "id": "gXRGKT3eF7pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(\n",
        "    df[\"Cluster_Label\"],\n",
        "    df[\"Final_Result\"],\n",
        "    normalize=\"index\"\n",
        ")"
      ],
      "metadata": {
        "id": "dJBh94xTGYQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.scatterplot(\n",
        "    x=df[\"Average_Quiz_Score\"],\n",
        "    y=df[\"Engagement_Index\"],\n",
        "    hue=df[\"Cluster_Label\"],\n",
        "    palette=\"Set1\"\n",
        ")\n",
        "\n",
        "plt.title(\"Student Clusters Visualization\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zenzNJEJGfQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"Cluster_Label\")[[\n",
        "    \"Average_Quiz_Score\",\n",
        "    \"Engagement_Index\",\n",
        "    \"Attendance\"\n",
        "]].mean()"
      ],
      "metadata": {
        "id": "WH29P-7vGh0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATE RECOMMENDATIONS"
      ],
      "metadata": {
        "id": "MR9jQOubH4qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Predicted_Result\"] = final_model.predict(scaler.transform(\n",
        "    df.drop([\"Final_Result\", \"Cluster\", \"Cluster_Label\"], axis=1)\n",
        "))"
      ],
      "metadata": {
        "id": "aZWGvDZ5H6ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Predicted_Label\"] = df[\"Predicted_Result\"].map({\n",
        "    0: \"Fail\",\n",
        "    1: \"Pass\"\n",
        "})"
      ],
      "metadata": {
        "id": "faYhcBEfH6ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendation(row):\n",
        "\n",
        "    if row[\"Predicted_Label\"] == \"Fail\":\n",
        "\n",
        "        if row[\"Cluster_Label\"] == \"At Risk (Low Engagement)\":\n",
        "            return \"Strong intervention: counseling + attendance monitoring.\"\n",
        "\n",
        "        elif row[\"Cluster_Label\"] == \"Struggling but Engaged\":\n",
        "            return \"Provide tutoring and academic mentoring.\"\n",
        "\n",
        "        elif row[\"Cluster_Label\"] == \"High Performer\":\n",
        "            return \"Unexpected risk: review academic records.\"\n",
        "\n",
        "    else:  # Predicted Pass\n",
        "\n",
        "        if row[\"Cluster_Label\"] == \"At Risk (Low Engagement)\":\n",
        "            return \"Monitor engagement; prevent future decline.\"\n",
        "\n",
        "        elif row[\"Cluster_Label\"] == \"Struggling but Engaged\":\n",
        "            return \"Strengthen weak concepts through extra practice.\"\n",
        "\n",
        "        elif row[\"Cluster_Label\"] == \"High Performer\":\n",
        "            return \"Encourage advanced learning opportunities.\"\n",
        "\n",
        "    return \"No action required.\""
      ],
      "metadata": {
        "id": "ay-1JQPwH6f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Recommendation\"] = df.apply(generate_recommendation, axis=1)"
      ],
      "metadata": {
        "id": "-LDYDdNGIC2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Models"
      ],
      "metadata": {
        "id": "AswphssAJI2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "dK1f3lU0IgWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(final_model, \"classifier.joblib\")"
      ],
      "metadata": {
        "id": "VxDkthCWJOYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, \"scaler.joblib\")"
      ],
      "metadata": {
        "id": "vXDqEoMBJT6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(kmeans, \"cluster_model.joblib\")"
      ],
      "metadata": {
        "id": "mC9ZslnlJZaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"classifier.joblib\")\n",
        "files.download(\"scaler.joblib\")\n",
        "files.download(\"cluster_model.joblib\")"
      ],
      "metadata": {
        "id": "vJZ-angTJaoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlGCcp_rKRNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}