{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vipW_DbzLCrp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Aniket-bit7/learning-analytics-ml-system/main/data/students_data.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Final_Result\"].unique()"
      ],
      "metadata": {
        "id": "1K5YiymvC8jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", df.shape)"
      ],
      "metadata": {
        "id": "yVIkKOlvAqL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pZOClY32Apc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping Student_ID from modeling as it has no predictive meaning and it may introduce noise\n",
        "df = df.drop(columns=[\"Student_ID\"])"
      ],
      "metadata": {
        "id": "I_VXsoa9DVQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking missing values -->\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "lW5cx8AKBauP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking duplicates -->\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "l7ujkB_CBaro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Mlt3_5Q4Bao5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization part -->\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df.hist(figsize=(12, 8))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eEi1oQDBFQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"Final_Result\", data=df)\n",
        "plt.title(\"Final Result Distribution\")\n",
        "plt.show()\n",
        "\n",
        "df[\"Final_Result\"].value_counts()"
      ],
      "metadata": {
        "id": "ZOD2byLMFbks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"Final_Result\", y=\"Attendance\", data=df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGRJs_hwI_58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Missing Values\n"
      ],
      "metadata": {
        "id": "kMjqpfYwN_x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before handling missing values -->\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df[\"Quiz2\"].fillna(df[\"Quiz2\"].mean(), inplace=True)\n",
        "df[\"Time_Spent\"].fillna(df[\"Time_Spent\"].mean(), inplace=True)\n",
        "df[\"Attendance\"].fillna(df[\"Attendance\"].mean(), inplace=True)\n",
        "\n",
        "# after handling missing values -->\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "uzLdSOytI_pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Handling"
      ],
      "metadata": {
        "id": "V2xagEMa3q6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df[\"Time_Spent\"].quantile(0.25)\n",
        "Q3 = df[\"Time_Spent\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df[\"Time_Spent\"] = df[\"Time_Spent\"].clip(lower_bound, upper_bound)"
      ],
      "metadata": {
        "id": "G4lbdyO63wmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=df[\"Time_Spent\"])\n",
        "plt.title(\"Time_Spent After Outlier Handling\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "89Anco9J4K5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding and visualization"
      ],
      "metadata": {
        "id": "PsMciOTdP8qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the 'Final_Result' column to numerical values\n",
        "df['Final_Result'] = df['Final_Result'].map({'Pass': 1, 'Fail': 0})\n",
        "\n",
        "# correlation heatmap -->\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BTeoO1tUP69c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "gEgORr0US0cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total Quiz Score -->\n",
        "df[\"Total_Quiz_Score\"] = df[\"Quiz1\"] + df[\"Quiz2\"] + df[\"Quiz3\"]"
      ],
      "metadata": {
        "id": "T-7IfPceS4VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average Quiz Score -->\n",
        "df[\"Average_Quiz_Score\"] = df[\"Total_Quiz_Score\"] / 3"
      ],
      "metadata": {
        "id": "unk9qA0iS4RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz_Consistency -->\n",
        "df[\"Quiz_Std\"] = df[[\"Quiz1\", \"Quiz2\", \"Quiz3\"]].std(axis=1)"
      ],
      "metadata": {
        "id": "-9t22yGOTFF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nc-5MpGBUl3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average Quiz Score -->\n",
        "df[\"Average_Quiz_Score\"] = df[\"Total_Quiz_Score\"] / 3"
      ],
      "metadata": {
        "id": "H6z2jmOpzKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz_Consistency -->\n",
        "df[\"Quiz_Std\"] = df[[\"Quiz1\", \"Quiz2\", \"Quiz3\"]].std(axis=1)"
      ],
      "metadata": {
        "id": "0iUX9GrZzKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Engagement_Index -->\n",
        "df[\"Engagement_Index\"] = (\n",
        "    df[\"Time_Spent\"] * 0.5 +\n",
        "    df[\"Assignments\"] * 0.3 +\n",
        "    df[\"Attendance\"] * 0.2\n",
        ")"
      ],
      "metadata": {
        "id": "n_aNBV-4zKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "7mytXKQFUl1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Ratio -->\n",
        "df[\"Quiz_Percentage\"] = (df[\"Total_Quiz_Score\"] / 300) * 100"
      ],
      "metadata": {
        "id": "l319YLz4UlyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Effort_Performance_Ratio -->\n",
        "df[\"Effort_Performance_Ratio\"] = (\n",
        "    df[\"Total_Quiz_Score\"] / (df[\"Time_Spent\"] + 1)\n",
        ")"
      ],
      "metadata": {
        "id": "NzWoIcXWUluJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset"
      ],
      "metadata": {
        "id": "gJZc7R6zzNrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[\"Final_Result\"])\n",
        "y = df[\"Final_Result\"]"
      ],
      "metadata": {
        "id": "DGa7l6BhzT4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "DO536tbXzT1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original distribution:\\n\", y.value_counts(normalize=True))\n",
        "print(\"\\nTraining distribution:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"\\nTest distribution:\\n\", y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "2hDcqPbpzTyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale Features"
      ],
      "metadata": {
        "id": "N9DJhsXp-p-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n"
      ],
      "metadata": {
        "id": "F8a6JZG4-utB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7_bcIwZ5-uqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING\n"
      ],
      "metadata": {
        "id": "aFaICeOH_nQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using Logistic regression for training model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "3OEuvFxT-uKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions -->\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]"
      ],
      "metadata": {
        "id": "5oLwxagM-uHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics -->\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "id": "C08GfaC5AMFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Class Imbalance Properly -->\n",
        "log_reg_balanced = LogisticRegression(\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg_balanced.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "GPkT26KSAh2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again evaluating -->\n",
        "y_pred_bal = log_reg_balanced.predict(X_test_scaled)\n",
        "y_prob_bal = log_reg_balanced.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Confusion Matrix (Balanced):\\n\", confusion_matrix(y_test, y_pred_bal))\n",
        "print(\"\\nClassification Report (Balanced):\\n\", classification_report(y_test, y_pred_bal))\n",
        "print(\"ROC-AUC Score (Balanced):\", roc_auc_score(y_test, y_prob_bal))"
      ],
      "metadata": {
        "id": "IrIGZ5ZLAlyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = log_reg_balanced"
      ],
      "metadata": {
        "id": "xTeTmpQdCJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LF-QxcjuCLek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}